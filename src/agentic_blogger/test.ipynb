{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import DuckDuckGoSearchResults, DuckDuckGoSearchRun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "search = DuckDuckGoSearchRun()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ollama is an open-source app that lets you run, create, and share large language models locally with a command-line interface. Learn why running LLMs locally is a game changer, what models Ollama supports, and how to get started with it. Ollama, an open-source tool, facilitates local or server-based language model integration, allowing free usage of Meta's Llama2 models. The installation process on Windows is explained, and ... The next step is to invoke Langchain to instantiate Ollama (with the model of your choice), and construct the prompt template. The usage of the cl.user_session is to mostly maintain the separation of user contexts and histories, which just for the purposes of running a quick demo, is not strictly required.. Chain is a Langchain interface called Runnable that is used to create custom chains. Ollama is a free and open-source application that allows you to run various large language models, including Llama 3, on your own computer, even with limited resources. Ollama takes advantage of the performance gains of llama.cpp, an open source library designed to allow you to run LLMs locally with relatively low hardware requirements. llama.cpp and ollama are efficient C++ implementations of the LLaMA language model that allow developers to run large language models on consumer-grade hardware, making them more accessible, cost-effective, and easier to integrate into various applications and research projects.. What's llama.cpp? llama.cpp is an open-source, lightweight, and efficient implementation of the LLaMA language ...\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "search.run(\"What is ollama?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = DuckDuckGoSearchResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(\"[snippet: Today, we're introducing Meta Llama 3, the next generation of our \"\n",
      " 'state-of-the-art open source large language model. Llama 3 models will soon '\n",
      " 'be available on AWS, Databricks, Google Cloud, Hugging Face, Kaggle, IBM '\n",
      " 'WatsonX, Microsoft Azure, NVIDIA NIM, and Snowflake, and with support from '\n",
      " 'hardware platforms offered by AMD, AWS, Dell, Intel ..., title: Introducing '\n",
      " 'Meta Llama 3: The most capable openly available LLM to date, link: '\n",
      " 'https://ai.meta.com/blog/meta-llama-3/], [snippet: Llama 3, the latest '\n",
      " \"version of Meta's large language model, has been introduced in two models, \"\n",
      " 'boasting 8 billion and 70 billion parameters, designed to redefine '\n",
      " 'processing power, versatility and accessibility. Unlike its predecessors, '\n",
      " 'Llama 3 is open source. Social media. Features like real-time language '\n",
      " 'translation and high-resolution image ..., title: What You Need to Know '\n",
      " \"about Meta AI's Llama 3 | Built In, link: \"\n",
      " 'https://builtin.com/articles/llama-3], [snippet: The Llama 3 release '\n",
      " 'introduces 4 new open LLM models by Meta based on the Llama 2 architecture. '\n",
      " 'They come in two sizes: 8B and 70B parameters, each with base (pre-trained) '\n",
      " 'and instruct-tuned versions. All the variants can be run on various types of '\n",
      " 'consumer hardware and have a context length of 8K tokens. Meta-Llama-3-8b: '\n",
      " \"Base 8B model., title: Welcome Llama 3 - Meta's new open LLM - Hugging Face, \"\n",
      " 'link: https://huggingface.co/blog/llama3], [snippet: A better assistant: '\n",
      " 'Thanks to our latest advances with Meta Llama 3, we believe Meta AI is now '\n",
      " \"the most intelligent AI assistant you can use for free - and it's available \"\n",
      " \"in more countries across our apps to help you plan dinner based on what's in \"\n",
      " 'your fridge, study for your test and so much more. More info: You can use '\n",
      " 'Meta AI in feed ..., title: Meet Your New Assistant: Meta AI, Built With '\n",
      " 'Llama 3 | Meta, link: '\n",
      " 'https://about.fb.com/news/2024/04/meta-ai-assistant-built-with-llama-3/]')\n"
     ]
    }
   ],
   "source": [
    "from pprint import pprint\n",
    "pprint(result.run(\"what is llama3?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import Tool\n",
    "from langchain.utilities import GoogleSerperAPIWrapper\n",
    "search = GoogleSerperAPIWrapper()\n",
    "\n",
    "# Create and assign the search tool to an agent\n",
    "serper_tool = Tool(\n",
    "  name=\"Intermediate Answer\",\n",
    "  func=search.run,\n",
    "  description=\"Useful for search-based queries\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
